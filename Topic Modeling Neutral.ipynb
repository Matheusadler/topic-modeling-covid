{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\mathe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation, TruncatedSVD\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining parameters\n",
    "n_features = 10000\n",
    "n_components = 10\n",
    "n_top_word = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to print topics\n",
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        message = \"Topic #%d: \" % topic_idx\n",
    "        message += \" \".join([feature_names[i]\n",
    "                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        print(message)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>User_name</th>\n",
       "      <th>Time</th>\n",
       "      <th>Location</th>\n",
       "      <th>text</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>London</td>\n",
       "      <td>@MeNyrbie @Phil_Gahan @Chrisitv https://t.co/i...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>Vagabonds</td>\n",
       "      <td>Coronavirus Australia: Woolworths to give elde...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>Austria</td>\n",
       "      <td>Was at the supermarket today. Didn't buy toile...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>Makati, Manila</td>\n",
       "      <td>All month there hasn't been crowding in the su...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>14</td>\n",
       "      <td>15</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ADARA Releases COVID-19 Resource Center for Tr...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  User_name        Time        Location  \\\n",
       "0           0          1  16-03-2020          London   \n",
       "1           2          3  16-03-2020       Vagabonds   \n",
       "2           7          8  16-03-2020         Austria   \n",
       "3          10         11  16-03-2020  Makati, Manila   \n",
       "4          14         15  16-03-2020             NaN   \n",
       "\n",
       "                                                text Sentiment  \n",
       "0  @MeNyrbie @Phil_Gahan @Chrisitv https://t.co/i...   Neutral  \n",
       "1  Coronavirus Australia: Woolworths to give elde...   Neutral  \n",
       "2  Was at the supermarket today. Didn't buy toile...   Neutral  \n",
       "3  All month there hasn't been crowding in the su...   Neutral  \n",
       "4  ADARA Releases COVID-19 Resource Center for Tr...   Neutral  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading Dataset\n",
    "print(\"Loading dataset...\")\n",
    "dataset = pd.read_csv('tweets-covid-neutral.csv', engine='python')\n",
    "data_samples = dataset.iloc[1:,0]\n",
    "n_samples = len(data_samples)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# StopWords\n",
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>User_name</th>\n",
       "      <th>Time</th>\n",
       "      <th>Location</th>\n",
       "      <th>text</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>London</td>\n",
       "      <td>@MeNyrbie @Phil_Gahan @Chrisitv https://t.co/i...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>Vagabonds</td>\n",
       "      <td>Coronavirus Australia: Woolworths to give elde...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>Austria</td>\n",
       "      <td>Was at the supermarket today. Didn't buy toile...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>Makati, Manila</td>\n",
       "      <td>All month there hasn't been crowding in the su...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>14</td>\n",
       "      <td>15</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ADARA Releases COVID-19 Resource Center for Tr...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  User_name        Time        Location  \\\n",
       "0           0          1  16-03-2020          London   \n",
       "1           2          3  16-03-2020       Vagabonds   \n",
       "2           7          8  16-03-2020         Austria   \n",
       "3          10         11  16-03-2020  Makati, Manila   \n",
       "4          14         15  16-03-2020             NaN   \n",
       "\n",
       "                                                text Sentiment  \n",
       "0  @MeNyrbie @Phil_Gahan @Chrisitv https://t.co/i...   Neutral  \n",
       "1  Coronavirus Australia: Woolworths to give elde...   Neutral  \n",
       "2  Was at the supermarket today. Didn't buy toile...   Neutral  \n",
       "3  All month there hasn't been crowding in the su...   Neutral  \n",
       "4  ADARA Releases COVID-19 Resource Center for Tr...   Neutral  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['menyrbie phil gahan chrisitv',\n",
       " 'coronavirus australia woolworths give elderly disabled dedicated shopping hours amid covid outbreak',\n",
       " 'supermarket today buy toilet paper rebel toiletpapercrisis covid',\n",
       " 'month crowding supermarkets restaurants however reducing hours closing malls means everyone using entrance dependent single supermarket manila lockdown covid philippines',\n",
       " 'adara releases covid resource center travel brands insights help travel brands stay date consumer travel behavior trends']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_comment = []\n",
    "# Pre-processing Positive\n",
    "for i in range(dataset.shape[0]):\n",
    "    comment = re.sub('[^a-zA-Z]',' ',dataset['text'][i]) # Remove non-letters\n",
    "    comment = comment.split(\"http\", 1)[0] # Remove address from string\n",
    "    comment = comment.lower() # Set lower case\n",
    "    comment = comment.split() # Divide into a list\n",
    "    comment = [word for word in comment if not word in stopwords.words('english')] # Select important words\n",
    "    comment =' '.join(comment)\n",
    "    new_comment.append(comment)\n",
    "\n",
    "new_comment[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting tf-idf features for NMF...\n",
      "tf-idf features extracted!\n"
     ]
    }
   ],
   "source": [
    "# Use tf-idf features for NMF.\n",
    "print(\"Extracting tf-idf features for NMF...\")\n",
    "tfidf_vectorizer = CountVectorizer(max_df=0.95, min_df=2,\n",
    "                                max_features=n_features,\n",
    "                                stop_words=stopwords.words('english'))\n",
    "tfidf = tfidf_vectorizer.fit_transform(new_comment)\n",
    "\n",
    "print(\"tf-idf features extracted!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting tf features for LDA...\n",
      "tf features for LDA extraction is completed!\n"
     ]
    }
   ],
   "source": [
    "# Use tf features for LDA.\n",
    "print(\"Extracting tf features for LDA...\")\n",
    "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2,\n",
    "                                max_features=n_features,\n",
    "                                stop_words=stopwords.words('english'))\n",
    "tf = tf_vectorizer.fit_transform(new_comment)\n",
    "\n",
    "print(\"tf features for LDA extraction is completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting the NMF model (Frobenius norm) with tf-idf features, n_samples=7373 and n_features=10000...\n",
      "done in 1.071s.\n",
      "\n",
      "Topics in NMF model (Frobenius norm):\n",
      "Topic #0: covid corona coronavirusoutbreak uk coronaviruspandemic quarantine coronacrisis\n",
      "Topic #1: coronavirus toiletpaper pandemic sanitizer lockdown quarantine outbreak\n",
      "Topic #2: store grocery go retail socialdistancing line workers\n",
      "Topic #3: supermarket go uk local one get socialdistancing\n",
      "Topic #4: shopping online pandemic new stores home grocery\n",
      "Topic #5: prices oil gas market due fall impact\n",
      "Topic #6: consumer behavior pandemic impact new amp trends\n",
      "Topic #7: toilet paper toiletpaper roll rolls need quarantine\n",
      "Topic #8: food stock amp stores delivery demand due\n",
      "Topic #9: people home go social many outside line\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\mathe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:1077: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  \" improve convergence.\" % max_iter, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "# Fit the NMF model\n",
    "print(\"Fitting the NMF model (Frobenius norm) with tf-idf features, \"\n",
    "      \"n_samples=%d and n_features=%d...\"\n",
    "      % (n_samples, n_features))\n",
    "t0 = time()\n",
    "nmf = NMF(n_components=n_components, random_state=1,\n",
    "          alpha=.1, l1_ratio=.5).fit(tfidf)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "print(\"\\nTopics in NMF model (Frobenius norm):\")\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "print_top_words(nmf, tfidf_feature_names, n_top_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting the NMF model (generalized Kullback-Leibler divergence) with tf-idf features, n_samples=7373 and n_features=10000...\n",
      "done in 1.952s.\n",
      "\n",
      "Topics in NMF model (generalized Kullback-Leibler divergence):\n",
      "Topic #0: covid coronavirus corona toiletpaper pandemic coronavirusoutbreak lockdown\n",
      "Topic #1: coronavirus sanitizer pandemic hand time could right\n",
      "Topic #2: store grocery retail workers going today go\n",
      "Topic #3: supermarket uk local shelves one workers shoppers\n",
      "Topic #4: shopping online consumer behavior new pandemic trends\n",
      "Topic #5: prices oil gas market due fall rise\n",
      "Topic #6: consumer impact pandemic new amp latest us\n",
      "Topic #7: toilet toiletpaper paper need last quarantine roll\n",
      "Topic #8: food stores stock amp due delivery closed\n",
      "Topic #9: people go get home going social socialdistancing\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Fit the NMF model\n",
    "print(\"Fitting the NMF model (generalized Kullback-Leibler divergence) with \"\n",
    "      \"tf-idf features, n_samples=%d and n_features=%d...\"\n",
    "      % (n_samples, n_features))\n",
    "t0 = time()\n",
    "nmf = NMF(n_components=n_components, random_state=1,\n",
    "          beta_loss='kullback-leibler', solver='mu', max_iter=1000, alpha=.1,\n",
    "          l1_ratio=.5).fit(tfidf)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "print(\"\\nTopics in NMF model (generalized Kullback-Leibler divergence):\")\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "print_top_words(nmf, tfidf_feature_names, n_top_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting LDA models with tf features, n_samples=7373 and n_features=10000...\n",
      "done in 9.688s.\n",
      "\n",
      "Topics in LDA model:\n",
      "Topic #0: coronavirus covid supermarket toiletpaper toilet paper people\n",
      "Topic #1: gt five fast increases inflation canadian narendramodi\n",
      "Topic #2: online shopping covid pandemic coronavirus amid outbreak\n",
      "Topic #3: store grocery coronavirus covid today going go\n",
      "Topic #4: covid coronavirus food consumer new home stock\n",
      "Topic #5: consumer covid impact behavior coronavirus via data\n",
      "Topic #6: coronavirus covid sanitizer amp stayathome got stayhome\n",
      "Topic #7: supermarket need covid week food supply products\n",
      "Topic #8: prices covid coronavirus oil gas face india\n",
      "Topic #9: coronavirus supermarket social workers due distancing march\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Fit the LDA model\n",
    "print(\"Fitting LDA models with tf features, \"\n",
    "      \"n_samples=%d and n_features=%d...\"\n",
    "      % (n_samples, n_features))\n",
    "lda = LatentDirichletAllocation(n_components=n_components, max_iter=5,\n",
    "                                learning_method='online',\n",
    "                                learning_offset=50.,\n",
    "                                random_state=0)\n",
    "t0 = time()\n",
    "lda.fit(tf)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "print(\"\\nTopics in LDA model:\")\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "print_top_words(lda, tf_feature_names, n_top_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fiting LSA model\n",
      "\n",
      "Topics in LSA model:\n",
      "Topic #0: covid coronavirus store supermarket grocery prices consumer\n",
      "Topic #1: covid consumer behavior impact response changes report\n",
      "Topic #2: store grocery go covid retail line socialdistancing\n",
      "Topic #3: supermarket people go get socialdistancing local social\n",
      "Topic #4: shopping online consumer behavior pandemic new retail\n",
      "Topic #5: consumer behavior coronavirus toiletpaper store impact toilet\n",
      "Topic #6: toiletpaper covid paper toilet coronavirus corona quarantine\n",
      "Topic #7: toilet paper toiletpaper people prices consumer amp\n",
      "Topic #8: food stock amp stores people delivery demand\n",
      "Topic #9: people grocery go toiletpaper get consumer line\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Fit the LSA model\n",
    "print(\"Fiting LSA model\")\n",
    "\n",
    "lsa = TruncatedSVD(n_components=n_components, n_iter=40, tol=0.01)\n",
    "\n",
    "lsa.fit(tf)\n",
    "\n",
    "print(\"\\nTopics in LSA model:\")\n",
    "\n",
    "print_top_words(lsa, tf_feature_names, n_top_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
